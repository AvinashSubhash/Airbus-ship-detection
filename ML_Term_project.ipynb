{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gcs4JrCV3lgL",
        "outputId": "940e3dab-0768-457e-f67b-462746b3441d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nMOFtJN5C27e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import random\n",
        "import skimage.color\n",
        "import skimage.filters\n",
        "import skimage.io\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split  \n",
        "import keras\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization \n",
        "from keras.optimizers import adam_v2, rmsprop_v2\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr4UpcV8b1W-"
      },
      "source": [
        "**Storing classification images into a CSV file**\n",
        "\n",
        "To mimic the training dataset available in the dataset on kaggle, we manually classified the dataset and then stored the data in a csv file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "v7SiB_TE31jj",
        "outputId": "6cc505f4-b6ec-4e10-bb62-c5b7c85168c9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'categories = []\\nfilenames = os.listdir(\"drive/MyDrive/final_data/yes\")\\nfor filename in filenames:\\n  categories.append(1)\\nfilenames2 = os.listdir(\"drive/MyDrive/final_data/no\")\\nfor filename in filenames2:\\n  categories.append(0)\\nfilenames.extend(filenames2)\\nprint(len(filenames),len(categories))\\ndf = pd.DataFrame({\\n    \\'filename\\': filenames,\\n    \\'category\\': categories})\\ndf.to_csv(\\'final_image_classes.csv\\')\\n'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''categories = []\n",
        "filenames = os.listdir(\"drive/MyDrive/final_data/yes\")\n",
        "for filename in filenames:\n",
        "  categories.append(1)\n",
        "filenames2 = os.listdir(\"drive/MyDrive/final_data/no\")\n",
        "for filename in filenames2:\n",
        "  categories.append(0)\n",
        "filenames.extend(filenames2)\n",
        "print(len(filenames),len(categories))\n",
        "df = pd.DataFrame({\n",
        "    'filename': filenames,\n",
        "    'category': categories})\n",
        "df.to_csv('final_image_classes.csv')\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "g1o7iK-9QBDi"
      },
      "outputs": [],
      "source": [
        "#!cp -a drive/MyDrive/final_data/no/. drive/MyDrive/final_data/total/\n",
        "#!cp -a drive/MyDrive/final_data/yes/. drive/MyDrive/final_data/total/ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kc2NC_TLj1H"
      },
      "source": [
        "**Data Loading in 2 Phase**\n",
        "\n",
        "Phase 1 is where we load the filenames in csv weith its according class. As our dataset was not having classes we had to manually classify the images to folders\n",
        "\n",
        "Phase 2 is where we use the csv file to load images from the image pool."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvZ5Z3YNcGXM"
      },
      "source": [
        "**Phase 1**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Bk_OYz5v-6-g"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('drive/MyDrive/final_image_classes.csv')\n",
        "df['category'] = df['category'].astype(str)\n",
        "df.head()\n",
        "train_df, validate_df = train_test_split(df, test_size=0.20, random_state=42)\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "validate_df = validate_df.reset_index(drop=True)\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    rescale=1./255,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KswAWZnUMPyB"
      },
      "source": [
        "**Phase 2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7aeIV6YEePz",
        "outputId": "686c3d99-5aac-4afb-f2bc-8b12e469e7f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2512 validated image filenames belonging to 2 classes.\n",
            "Found 628 validated image filenames belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    train_df, \n",
        "    \"drive/MyDrive/final_data/total/\",\n",
        "    x_col='filename',\n",
        "    #color_mode = 'grayscale',\n",
        "    y_col='category',\n",
        "    target_size=(256,256),\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "validation_generator = validation_datagen.flow_from_dataframe(\n",
        "    validate_df, \n",
        "    \"drive/MyDrive/final_data/total/\", \n",
        "    x_col='filename',\n",
        "    y_col='category',\n",
        "    shuffle=False,\n",
        "    target_size=(256,256),\n",
        "    class_mode='categorical')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d1JdgFnMUJE"
      },
      "source": [
        "**Model Preperation**\n",
        "\n",
        "For this CNN model we have kept the pool size fixed to (2,2)\n",
        "We have used softmax function at the final layer for two outputs for observing the probability ofboth the classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OUZfwlaBNl3D"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3, 3), input_shape=(256,256,3),activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(2, activation='softmax'))          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfLypmZwO7yl",
        "outputId": "0615547a-f9e1-4653-daea-b4ae05e9db8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_1 (Conv2D)           (None, 254, 254, 32)      896       \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 254, 254, 32)     128       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 127, 127, 32)     0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 127, 127, 32)      0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 125, 125, 64)      18496     \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 125, 125, 64)     256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 62, 62, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 62, 62, 64)        0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 60, 60, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 60, 60, 128)      512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 30, 30, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 30, 30, 128)       0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 115200)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               14745728  \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 128)              512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,840,642\n",
            "Trainable params: 14,839,938\n",
            "Non-trainable params: 704\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4kTU29QFcsJ",
        "outputId": "91e6bf73-0a37-4ab1-8cd5-123dd27845e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_generator, \n",
        "    epochs=20,\n",
        "    validation_data=validation_generator,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsymlNHRaUI5"
      },
      "source": [
        "**Accuracy and Loss Trend during training**\n",
        "\n",
        "According to the graphs, and the output after each epoch, it can be observed that although over accuracy and loss are gradually going towards the right direction, the validation data is fluctuating a lot.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pp8oQWoDXgyl"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
        "print(\"Accuracy  &  Validation Accuracy\\n\")\n",
        "ax1.title.set_text(\"Accuracy  &  Validation Accuracy\\n\")\n",
        "ax1.plot(history.history['accuracy'], color='g', label=\"Training accuracy\")\n",
        "ax1.plot(history.history['val_accuracy'], color='y',label=\"Validation accuracy\")\n",
        "ax2.title.set_text(\"Loss  &  Validation Loss\\n\")\n",
        "ax2.plot(history.history['loss'], color='g', label=\"Training loss\")\n",
        "ax2.plot(history.history['val_loss'], color='y', label=\"validation loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsay2TK3MwuT"
      },
      "source": [
        "**Storing Weights**\n",
        "\n",
        "To avoid training the model everytime to show the performance and outputs, we have saved the model in h5 format. The training this particular model takes 37 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdLu9ZSOphXZ"
      },
      "outputs": [],
      "source": [
        "model.save_weights('final_model_weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xG2t0-uZrVxS"
      },
      "outputs": [],
      "source": [
        "model.load_weights('drive/MyDrive/final_model_weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-FwVmZzE9XUj"
      },
      "outputs": [],
      "source": [
        "out_p = model.predict(validation_generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emSiL2peGaQy"
      },
      "outputs": [],
      "source": [
        "out_p = out_p > 0.5\n",
        "out_p = out_p.astype(int)\n",
        "import matplotlib.image as mpimg\n",
        "for i in range(60,80):\n",
        "  img = mpimg.imread('drive/MyDrive/final_data/total/'+validate_df['filename'][i])\n",
        "  imgplot = plt.imshow(img)\n",
        "  plt.show()\n",
        "  print(\"Actual: \",validate_df['category'][i],\" Prediction: \",out_p[i])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "ML_Term_project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
